XtrainFact = select(Xtrain, state)
%>%
mutate_all(factor)
XtrainFact = select(Xtrain, 'state')
%>%
mutate_all(factor)
XtrainFact = select(Xtrain, c('state'),
) %>%
mutate_all(factor)
dummyModel = dummyVars(~ ., data = XtrainFact, fullRank = TRUE)
XtrainQualDummy = predict(dummyModel, XtrainFact)
XtrainQualDummy
XtrainQuan = select(Xtrain, county,location_combined,fips,date,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainQuan
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
require(corrplot)
corrplot(cor(XtrainFull), tl.cex = 0.5)
XtrainQuan = select(Xtrain,date,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
#corrplot
require(corrplot)
corrplot(cor(XtrainFull), tl.cex = 0.5)
XtrainQuan2 = select(Xtrain,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull2 = XtrainFull = cbind(XtrainQualDummy, XtrainQuan2)
#corrplot
require(corrplot)
corrplot(cor(XtrainFull2), tl.cex = 0.5)
getwd()
basename(getwd())
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(reshape2)){install.packages('reshape2');require(reshape2)}
# The file paths below assume your working directory is 'stats' (where this file is located)
if(basename(getwd())=="COVID-19"){df_Mobility = read.csv("./stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
if(basename(getwd())=="stats"){df_Mobility = read.csv("./stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
df_COVID_conf_US_resh = melt(df_COVID_conf_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key"),na.rm=FALSE)
df_COVID_death_US_resh = melt(df_COVID_death_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key","Population"),na.rm=FALSE)
# Next step is to parse some dates in both tables for the join
# Parsing all dates as date_parsed, dropping raw date, renaming associated value
df_Mobility$date_parsed = as.Date(df_Mobility$date,format='%Y-%m-%d')
df_COVID_conf_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_conf_US_resh$variable),'\\.','/'),start=2)
df_COVID_conf_US_resh$date_parsed = as.Date(df_COVID_conf_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_conf_US_resh = subset(df_COVID_conf_US_resh,select=-variable)
names(df_COVID_conf_US_resh)[names(df_COVID_conf_US_resh)=="value"] = "cases"
df_COVID_death_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_death_US_resh$variable),'\\.','/'),start=2)
df_COVID_death_US_resh$date_parsed = as.Date(df_COVID_death_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_death_US_resh = subset(df_COVID_death_US_resh,select=-variable)
names(df_COVID_death_US_resh)[names(df_COVID_death_US_resh)=="value"] = "deaths"
# Joining all three tables using the 'conf' table as the base
# First join is between the time series DFs on all variables available since it's the same source
# The first inner join causes a loss of 2.3% of rows (small county diffs)
# Second join is between the new time series DF and the mobility DF on FIPS and Date
# The second inner join causes a loss of 17.6% (small county and time period diffs)
df_intermediate = inner_join(df_COVID_conf_US_resh,df_COVID_death_US_resh)
df_intermediate$FIPS = as.character(df_intermediate$FIPS)
df_Mobility$census_fips_code = as.character(df_Mobility$census_fips_code)
df_merged = inner_join(df_intermediate,df_Mobility,by=c('date_parsed'='date_parsed',"FIPS"='census_fips_code'))
# I'll select and rename columns for all further analysis while leaving the joined table alone
df_subset = subset(df_merged,
select=c(Country_Region,Province_State,Admin2,Combined_Key,FIPS,date_parsed,Population,cases,deaths,
retail_and_recreation_percent_change_from_baseline,grocery_and_pharmacy_percent_change_from_baseline,
parks_percent_change_from_baseline,transit_stations_percent_change_from_baseline,
workplaces_percent_change_from_baseline,residential_percent_change_from_baseline))
names(df_subset) = c('nation','state','county','location_combined','fips','date','population','cases','deaths',
'mobility_retail_recreation_change','mobility_grocery_pharmacy_change','mobility_parks_change',
'mobility_transit_stations_change','mobility_workplaces_change','mobility_residential_change')
require(dplyr)
require(readr)
require(caret)
require(pROC)
#treat all data as training for now
Xtrain = select(df_subset, -c('cases','deaths'))
Ytrain = select(df_subset, c('cases','deaths'))
#explore structure
head(Xtrain)
str(Xtrain)
#Note: we can put the number of unique values with the data structure:
rbind(sapply(Xtrain,function(x){ length(unique(x))}),
sapply(Xtrain,class))
str(Ytrain)
rbind(sapply(Ytrain,function(x){ length(unique(x))}),
sapply(Ytrain,class))
# Check to see if there are any missing values in the training data
anyNA(Xtrain)
# Note - NAs appear to be related to Google mobility data
# Google says treat the "gaps" as true unknowns and don't assume it means places weren't busy
# https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927
# Changes are measured in percent changes from baseline
# Consider an imputation based on that day of the week from surrounding weeks?
# or treat the NAs separately?
# view Missing for fun
ggplot_missing <- function(x){
if(!require(reshape2)){warning('you need to install reshape2')}
require(reshape2)
require(ggplot2)
x %>%
is.na %>%
melt %>%
ggplot(data = .,
aes(x = Var2,
y = Var1)) +
geom_raster(aes(fill = value)) +
scale_fill_grey(name = "",
labels = c("Present","Missing")) +
theme_minimal() +
theme(axis.text.x  = element_text(angle=45, vjust=0.5)) +
labs(x = "Variables in Dataset",
y = "Rows / observations")
}
ggplot_missing(Xtrain)
# Interesting, lots of missing FIPs, the rest happens to be google data which is more expected.
print(Xtrain[is.na(Xtrain$fips) & Xtrain$state != 'Massachusetts' & Xtrain$state != 'Michigan' & Xtrain$state != 'Missouri' & Xtrain$state != 'Utah',])
#skip imputation for now
#Do we want state and county dummies at all? Will just do state for now.
XtrainFact = select(Xtrain, c('state'),
) %>%
mutate_all(factor)
dummyModel = dummyVars(~ ., data = XtrainFact, fullRank = TRUE)
XtrainQualDummy = predict(dummyModel, XtrainFact)
XtrainQuan = select(Xtrain,date,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
XtrainQuan2 = select(Xtrain,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull2 = XtrainFull = cbind(XtrainQualDummy, XtrainQuan2)
#corrplot
require(corrplot)
corrplot(cor(XtrainFull2), tl.cex = 0.5)
?ts()
library(tidyverse)
library(tidyquant)
library(timetk)
library(sweep)
library(forecast)
if(!require(tidyquant)){install.packages('tidyquant');require(tidyquant)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(tidyquant)){install.packages('tidyquant');require(tidyquant)}
if(!require(timetk)){install.packages('timetk');require(timetk)}
if(!require(sweep)){install.packages('sweep');require(sweep)}
if(!require(forecast)){install.packages('forecast');require(forecast)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(tidyquant)){install.packages('tidyquant');require(tidyquant)}
if(!require(timetk)){install.packages('timetk');require(timetk)}
if(!require(sweep)){install.packages('sweep');require(sweep)}
if(!require(forecast)){install.packages('forecast');require(forecast)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(reshape2)){install.packages('reshape2');require(reshape2)}
# The file paths below assume your working directory is 'stats' (where this file is located)
if(basename(getwd())=="COVID-19"){df_Mobility = read.csv("./stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
if(basename(getwd())=="stats"){df_Mobility = read.csv("../stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
# Following stuff is just goofing around
# head(df_Mobility)
#
# summary(df_Mobility)
#
# table(df_Mobility[df_Mobility$sub_region_1=='Louisiana',]$sub_region_2)
#
# head(df_COVID_conf_US)
#
# head(df_COVID_death_US)
#
# table(df_COVID_conf_US$FIPS)
#
# # Curious about what Louisiana Parishes are listed as
#
# table(df_COVID_conf_US[df_COVID_conf_US$Province_State=='Louisiana',]$Admin2)
#end Goofs
#stripping off the word "Parish" and "County". I am 70% sure there will be other things to fix here when joining.
#df_Mobility$County_trim = word(df_Mobility$sub_region_2,1,-2)
#Need to do some date transposing before joining.
df_COVID_conf_US_resh = melt(df_COVID_conf_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key"),na.rm=FALSE)
df_COVID_death_US_resh = melt(df_COVID_death_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key","Population"),na.rm=FALSE)
# Next step is to parse some dates in both tables for the join
# Parsing all dates as date_parsed, dropping raw date, renaming associated value
df_Mobility$date_parsed = as.Date(df_Mobility$date,format='%Y-%m-%d')
df_COVID_conf_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_conf_US_resh$variable),'\\.','/'),start=2)
df_COVID_conf_US_resh$date_parsed = as.Date(df_COVID_conf_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_conf_US_resh = subset(df_COVID_conf_US_resh,select=-variable)
names(df_COVID_conf_US_resh)[names(df_COVID_conf_US_resh)=="value"] = "cases"
df_COVID_death_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_death_US_resh$variable),'\\.','/'),start=2)
df_COVID_death_US_resh$date_parsed = as.Date(df_COVID_death_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_death_US_resh = subset(df_COVID_death_US_resh,select=-variable)
names(df_COVID_death_US_resh)[names(df_COVID_death_US_resh)=="value"] = "deaths"
# Joining all three tables using the 'conf' table as the base
# First join is between the time series DFs on all variables available since it's the same source
# The first inner join causes a loss of 2.3% of rows (small county diffs)
# Second join is between the new time series DF and the mobility DF on FIPS and Date
# The second inner join causes a loss of 17.6% (small county and time period diffs)
df_intermediate = inner_join(df_COVID_conf_US_resh,df_COVID_death_US_resh)
df_intermediate$FIPS = as.character(df_intermediate$FIPS)
df_Mobility$census_fips_code = as.character(df_Mobility$census_fips_code)
df_merged = inner_join(df_intermediate,df_Mobility,by=c('date_parsed'='date_parsed',"FIPS"='census_fips_code'))
# I'll select and rename columns for all further analysis while leaving the joined table alone
df_subset = subset(df_merged,
select=c(Country_Region,Province_State,Admin2,Combined_Key,FIPS,date_parsed,Population,cases,deaths,
retail_and_recreation_percent_change_from_baseline,grocery_and_pharmacy_percent_change_from_baseline,
parks_percent_change_from_baseline,transit_stations_percent_change_from_baseline,
workplaces_percent_change_from_baseline,residential_percent_change_from_baseline))
names(df_subset) = c('nation','state','county','location_combined','fips','date','population','cases','deaths',
'mobility_retail_recreation_change','mobility_grocery_pharmacy_change','mobility_parks_change',
'mobility_transit_stations_change','mobility_workplaces_change','mobility_residential_change')
require(dplyr)
require(readr)
require(caret)
require(pROC)
#treat all data as training for now
Xtrain = select(df_subset, -c('cases','deaths'))
Ytrain = select(df_subset, c('cases','deaths'))
#explore structure
head(Xtrain)
str(Xtrain)
#Note: we can put the number of unique values with the data structure:
rbind(sapply(Xtrain,function(x){ length(unique(x))}),
sapply(Xtrain,class))
str(Ytrain)
rbind(sapply(Ytrain,function(x){ length(unique(x))}),
sapply(Ytrain,class))
# Check to see if there are any missing values in the training data
anyNA(Xtrain)
# Note - NAs appear to be related to Google mobility data
# Google says treat the "gaps" as true unknowns and don't assume it means places weren't busy
# https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927
# Changes are measured in percent changes from baseline
# Consider an imputation based on that day of the week from surrounding weeks?
# or treat the NAs separately?
# view Missing for fun
ggplot_missing <- function(x){
if(!require(reshape2)){warning('you need to install reshape2')}
require(reshape2)
require(ggplot2)
x %>%
is.na %>%
melt %>%
ggplot(data = .,
aes(x = Var2,
y = Var1)) +
geom_raster(aes(fill = value)) +
scale_fill_grey(name = "",
labels = c("Present","Missing")) +
theme_minimal() +
theme(axis.text.x  = element_text(angle=45, vjust=0.5)) +
labs(x = "Variables in Dataset",
y = "Rows / observations")
}
ggplot_missing(Xtrain)
# Interesting, lots of missing FIPs, the rest happens to be google data which is more expected.
print(Xtrain[is.na(Xtrain$fips) & Xtrain$state != 'Massachusetts' & Xtrain$state != 'Michigan' & Xtrain$state != 'Missouri' & Xtrain$state != 'Utah',])
#skip imputation for now
#Do we want state and county dummies at all? Will just do state for now.
XtrainFact = select(Xtrain, c('state'),
) %>%
mutate_all(factor)
dummyModel = dummyVars(~ ., data = XtrainFact, fullRank = TRUE)
XtrainQualDummy = predict(dummyModel, XtrainFact)
XtrainQuan = select(Xtrain,date,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
XtrainQuan2 = select(Xtrain,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull2 = XtrainFull = cbind(XtrainQualDummy, XtrainQuan2)
#corrplot
require(corrplot)
corrplot(cor(XtrainFull2), tl.cex = 0.5)
head(XtrainFull)
head(XtrainQuan)
)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
XtrainFull3 = cbind(XtrainFact,XtrainQuan)
head(XtrainFull3)
Xtrain
as.yearmon()
?as.yearmon()
if(!require(zoo){install.packages('zoo');require(zoo)}
if(!require(zoo)){install.packages('zoo');require(zoo)}
?as.yearmon()
head(df_subset)
?summarise()
organized <- df_subset %>%
group_by(State, fips, date)
organized <- df_subset %>%
group_by(state, fips, date)
head(organized)
organized_nest <- organized  %>%
group_by(state, fips) %>%
nest()
organized_nest
?map()
head(df_subset)
organized_nest_ts <- rganized_nest %>%
mutate(data.ts = map(.x       = data,
.f       = tk_ts,
select   = -date,
start    = 1,
freq     = 7))
organized_nest_ts <- organized_nest %>%
mutate(data.ts = map(.x       = data,
.f       = tk_ts,
select   = -date,
start    = 1,
freq     = 7))
?tk_ts()
??tk_ts()
if(!require(timetk)){install.packages('timetk');require(timetk)}
organized_nest_ts <- organized_nest %>%
mutate(data.ts = map(.x       = data,
.f       = tk_ts,
select   = -date,
start    = 1,
freq     = 7))
head(organized_nest_ts)
organized_nest_ts$data
organized_nest_ts$data.ts
head(organized)
?create_ts()
??create_ts()
?pluck()
?with()
head(organized_nest_ts)
?unnest()
organized_unnest_ts <- organized_nest_ts %>%
unnest()
organized_unnest_ts <- organized_nest_ts %>%
unnest(cols=c(data,data.ts))
head(organized_nest_ts$data)
head(organized_nest_ts$data.ts)
organized_unnest_ts <- organized_nest_ts %>%
unnest(cols=c(data,data.ts))
if(!require(vctrs)){install.packages('vctrs');require(vctrs)}
organized_unnest_ts <- organized_nest_ts %>%
unchop()
head(organized_unnest_ts)
organized_unnest_ts <- organized_nest_ts %>%
unchop(c(data,data.ts))
organized_nest_ts <- organized_nest %>%
mutate(data.ts = map(.x       = data,
.f       = tk_ts,
select   = -date,
start    = 1,
freq     = 7)) %>%
select(-data)
head(organized_nest_ts)
head(organized_nest_ts$data.ts)
organized_unnest_ts <- organized_nest_ts %>%
unchop(data.ts)
organized_unnest_ts <- organized_nest_ts %>%
unnest(data.ts)
if(!require(purrr){install.packages('purrr');require(purrr)}
if(!require(purrr)){install.packages('purrr');require(purrr)}
organized_unnest_ts <- organized_nest_ts %>%
pmap_dfr(data.frame)
head(organized_unnest_ts)
head(organized_nest_ts)
head(data.ts)
head(organized_nest_ts$data.ts)
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(reshape2)){install.packages('reshape2');require(reshape2)}
# The file paths below assume your working directory is 'stats' (where this file is located)
if(basename(getwd())=="COVID-19"){df_Mobility = read.csv("./stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("./csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
if(basename(getwd())=="stats"){df_Mobility = read.csv("../stats/2020_US_Region_Mobility_Report.csv")
df_COVID_conf_US = read.csv("../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
df_COVID_death_US = read.csv("../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")}
# Following stuff is just goofing around
# head(df_Mobility)
#
# summary(df_Mobility)
#
# table(df_Mobility[df_Mobility$sub_region_1=='Louisiana',]$sub_region_2)
#
# head(df_COVID_conf_US)
#
# head(df_COVID_death_US)
#
# table(df_COVID_conf_US$FIPS)
#
# # Curious about what Louisiana Parishes are listed as
#
# table(df_COVID_conf_US[df_COVID_conf_US$Province_State=='Louisiana',]$Admin2)
#end Goofs
#stripping off the word "Parish" and "County". I am 70% sure there will be other things to fix here when joining.
#df_Mobility$County_trim = word(df_Mobility$sub_region_2,1,-2)
#Need to do some date transposing before joining.
df_COVID_conf_US_resh = melt(df_COVID_conf_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key"),na.rm=FALSE)
df_COVID_death_US_resh = melt(df_COVID_death_US,id.vars=c("UID","iso2","iso3","code3","FIPS","Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key","Population"),na.rm=FALSE)
# Next step is to parse some dates in both tables for the join
# Parsing all dates as date_parsed, dropping raw date, renaming associated value
df_Mobility$date_parsed = as.Date(df_Mobility$date,format='%Y-%m-%d')
df_COVID_conf_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_conf_US_resh$variable),'\\.','/'),start=2)
df_COVID_conf_US_resh$date_parsed = as.Date(df_COVID_conf_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_conf_US_resh = subset(df_COVID_conf_US_resh,select=-variable)
names(df_COVID_conf_US_resh)[names(df_COVID_conf_US_resh)=="value"] = "cases"
df_COVID_death_US_resh$date_parsed = str_sub(str_replace_all(as.character(df_COVID_death_US_resh$variable),'\\.','/'),start=2)
df_COVID_death_US_resh$date_parsed = as.Date(df_COVID_death_US_resh$date_parsed,format='%m/%d/%y')
df_COVID_death_US_resh = subset(df_COVID_death_US_resh,select=-variable)
names(df_COVID_death_US_resh)[names(df_COVID_death_US_resh)=="value"] = "deaths"
# Joining all three tables using the 'conf' table as the base
# First join is between the time series DFs on all variables available since it's the same source
# The first inner join causes a loss of 2.3% of rows (small county diffs)
# Second join is between the new time series DF and the mobility DF on FIPS and Date
# The second inner join causes a loss of 17.6% (small county and time period diffs)
df_intermediate = inner_join(df_COVID_conf_US_resh,df_COVID_death_US_resh)
df_intermediate$FIPS = as.character(df_intermediate$FIPS)
df_Mobility$census_fips_code = as.character(df_Mobility$census_fips_code)
df_merged = inner_join(df_intermediate,df_Mobility,by=c('date_parsed'='date_parsed',"FIPS"='census_fips_code'))
# I'll select and rename columns for all further analysis while leaving the joined table alone
df_subset = subset(df_merged,
select=c(Country_Region,Province_State,Admin2,Combined_Key,FIPS,date_parsed,Population,cases,deaths,
retail_and_recreation_percent_change_from_baseline,grocery_and_pharmacy_percent_change_from_baseline,
parks_percent_change_from_baseline,transit_stations_percent_change_from_baseline,
workplaces_percent_change_from_baseline,residential_percent_change_from_baseline))
names(df_subset) = c('nation','state','county','location_combined','fips','date','population','cases','deaths',
'mobility_retail_recreation_change','mobility_grocery_pharmacy_change','mobility_parks_change',
'mobility_transit_stations_change','mobility_workplaces_change','mobility_residential_change')
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
if(!require(tidyquant)){install.packages('tidyquant');require(tidyquant)}
if(!require(timetk)){install.packages('timetk');require(timetk)}
if(!require(sweep)){install.packages('sweep');require(sweep)}
if(!require(forecast)){install.packages('forecast');require(forecast)}
if(!require(zoo)){install.packages('zoo');require(zoo)}
if(!require(vctrs)){install.packages('vctrs');require(vctrs)}
if(!require(purrr)){install.packages('purrr');require(purrr)}
# just do a sort on state and fips, may need to eventually drop fips
organized <- df_subset %>%
group_by(state, fips, date)
# Nest into tables
organized_nest <- organized  %>%
group_by(state, fips) %>%
nest()
# mutate and map
organized_nest_ts <- organized_nest %>%
mutate(data.ts = map(.x       = data,
.f       = tk_ts,
select   = -date,
start    = 1,
freq     = 7)) %>%
select(-data)
head(organized_nest_ts)
head(organized_nest_ts$data.ts)
require(dplyr)
require(readr)
require(caret)
require(pROC)
#treat all data as training for now
Xtrain = select(df_subset, -c('cases','deaths'))
Ytrain = select(df_subset, c('cases','deaths'))
#explore structure
head(Xtrain)
str(Xtrain)
#Note: we can put the number of unique values with the data structure:
rbind(sapply(Xtrain,function(x){ length(unique(x))}),
sapply(Xtrain,class))
str(Ytrain)
rbind(sapply(Ytrain,function(x){ length(unique(x))}),
sapply(Ytrain,class))
# Check to see if there are any missing values in the training data
anyNA(Xtrain)
# Note - NAs appear to be related to Google mobility data
# Google says treat the "gaps" as true unknowns and don't assume it means places weren't busy
# https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927
# Changes are measured in percent changes from baseline
# Consider an imputation based on that day of the week from surrounding weeks?
# or treat the NAs separately?
# view Missing for fun
ggplot_missing <- function(x){
if(!require(reshape2)){warning('you need to install reshape2')}
require(reshape2)
require(ggplot2)
x %>%
is.na %>%
melt %>%
ggplot(data = .,
aes(x = Var2,
y = Var1)) +
geom_raster(aes(fill = value)) +
scale_fill_grey(name = "",
labels = c("Present","Missing")) +
theme_minimal() +
theme(axis.text.x  = element_text(angle=45, vjust=0.5)) +
labs(x = "Variables in Dataset",
y = "Rows / observations")
}
ggplot_missing(Xtrain)
# Interesting, lots of missing FIPs, the rest happens to be google data which is more expected.
print(Xtrain[is.na(Xtrain$fips) & Xtrain$state != 'Massachusetts' & Xtrain$state != 'Michigan' & Xtrain$state != 'Missouri' & Xtrain$state != 'Utah',])
#skip imputation for now
#Do we want state and county dummies at all? Will just do state for now.
XtrainFact = select(Xtrain, c('state'),
) %>%
mutate_all(factor)
dummyModel = dummyVars(~ ., data = XtrainFact, fullRank = TRUE)
XtrainQualDummy = predict(dummyModel, XtrainFact)
XtrainQuan = select(Xtrain,date,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull = cbind(XtrainQualDummy, XtrainQuan)
XtrainQuan2 = select(Xtrain,population,mobility_retail_recreation_change,mobility_grocery_pharmacy_change,mobility_parks_change,
mobility_transit_stations_change,mobility_workplaces_change,mobility_residential_change)
XtrainFull2= cbind(XtrainQualDummy, XtrainQuan2)
XtrainFull3 = cbind(XtrainFact,XtrainQuan)
head(XtrainFull)
?ets()
?tslm
